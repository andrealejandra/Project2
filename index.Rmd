---
title: 'Data Mining, Classification, Prediction'
author: "Andrea Virgen"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Oura Ring Biodata Analysis


### Introduction 

This dataset is a collection of health data from one individual measured with an Oura Ring over the course of 221 days. This wearable bio tracker collects a breadth of data: sleep, heart rate, breathing rate, body temperature, and physical activity. Oura software uses this information to calculate scores meant to inform the user of their sleep quality and its influence over other bio metrics. [https://ouraring.com/blog]

My intention is to explore how and if fluctuations in temperature readings can be predicted by lifestyle choices and/or biometrics.
Temperature readings are stored as standard deviations in Oura software, so I have categorized data points as either Increase or Decrease dependent on their orientation to the mean: there are 73 observations above and 103 observations below.


```{R}
library(tidyverse)
library(readr)
library(readxl)

biotrends <- read_csv("/Users/andi/Desktop/PORTFOLIO/Project2/oura_2020-12-05_2021-12-05_trends.csv")

#remove scores, NA values, and days with non-wear time
biotrends %>%
  select(-c(2:9,20:22,28:34,48:56)) %>%
  na.omit() %>%
  filter(`Non-wear Time`== 0) -> biotrends

biotrends %>%
  select(c(4:5,10,14:16,20,28)) -> biotrends

#create binary var
biotrends %>%
  mutate(`Temperature` = ifelse(`Temperature Deviation (°C)`>0, "Increase", "Decrease")) %>%
  select(-5) -> bioanalysis

table(bioanalysis$Temperature)

```

### Cluster Analysis

```{R}
library(cluster)
library(GGally)

# create subset of just numeric variables
bioanalysis %>%
  select_if(is.numeric) -> bio_num
#scale variables
bio_scale <- data.frame(scale(bio_num))

# calculate silhouette width and pick # of clusters
sil_width<-vector()
for(i in 2:10){
  pam_fit <- pam(bio_scale, k = i)  
  sil_width[i] <- pam_fit$silinfo$avg.width
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)

# pam w/ 3 clusters
bio_scale %>%
  pam(k=2) -> bio_pam

#results of pam
bio_pam

plot(bio_pam,which=2)
# visualize clusters in ggpairs
bio_scale %>%
  mutate(cluster=as.factor(bio_pam$clustering)) %>%
  ggpairs(columns = c(1:7), aes(color=cluster))

```
To determine the amount of clusters that is most reasonable for my dataset, I plotted silhouette widths for amount of clusters ranging from 2 to 10. The largest width was for 2 clusters with an average width of .19, but this does not reflect any significant structure within the dataset. The medoids were partitioned around data point 65 for cluster 1 and data point 103

Red and blue clusters(1 and 2, respectively) show the most distinction in variables Awake Time and REM Sleep Time. Cluster 1 encompasses lower Awake Time and REM Sleep Values as well as lowest Average HRV(Heart Rate Variability) values. Steps and Average MET have the strongest positive correlation in both clusters, and this is expected because steps are a form of physical expenditure. Average MET is the average metabolic equivalent where one MET would mean an individual is only expending enough metabolic energy to fuel their basal metabolic processes. Cluster 2(blue) shows the next strongest correlation and shows a negative relationship between Sleep Latency and REM Sleep Time. This is reasonable considering increased Sleep Latency should allow more opportunity for REM Sleep Time, and this finding is bolstered by the the third strongest positive correlation between Sleep Latency and Awake Time in cluster 2.

There were a few unexpected correlations: Average HRV showed a small positive correlation with Sleep Latency in cluster 2, Average MET and Awake Time were slightly negatively correlated in cluster 1, Respiratory Rate and Sleep Latency showed a slight negative correlation in cluster 2, and Respiratory Rate and Average HRV showed another slight negative correlation in cluster 2.

    
    
### Dimensionality Reduction with PCA

```{R}
library(factoextra)
# eig1<- bio %>% cor %>% eigen()
# eig1$vectors

# conduct pca with scaled data
bio_pca <- princomp(bio_scale)
summary(bio_pca, loadings=T)

# save eigen values and varience proportion for scree plot
eigval<-bio_pca$sdev^2
varprop=round(eigval/sum(eigval), 2)

# can compare these two and only keep PCs where eigval > 1
# eigval for each PC shows the correlation for each variable across the PC axis
eigval
varprop

ggplot() + geom_bar(aes(y=varprop, x=1:7), stat="identity") + xlab("") + geom_path() +
  geom_text(aes(x=1:7, y=varprop, label=round(varprop, 2)), vjust=1, col="white")+
  scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) + 
  scale_x_continuous(breaks=1:10)


fviz_pca_biplot(bio_pca)
```
In my PCA analysis of the numerical variables in my dataset, the scree plot was not helpful in trying to determine which Principle Components should be kept. I decided to look at PC 1-3 because they all have a Standard Deviation above 1 and account of a cumulative proportion of 70.2% of the variance across the 7 scaled, numeric variables. I decided to leave out the extra ~17% cumulative proportion accounted for in PC 4 because the limited interpretation accounting for only an extra ~14% of variance seemed unnecessary.

PC 1 showed the strongest negative correlations with the variables Steps and Average MET at approximately -.61 each. This PC also showed a negative correlation of -.31 in Average HRV(Heart Rate Variability).

PC2 showed the most positive relationships out of the 3 Principle Components and one negative with Average HRV. Awake Time has the highest positive correlation with PC2 at .51 with REM Sleep Time and Sleep Latency having the second and third largest positive correlations.

PC3 showed the strongest positive correlation in Sleep Latency. Average HRV and Respiratory Rate are inversely correlated across this PC at almost the same rate with Respiratory Rate being the negative counterpart. Awake Time and REM Sleep Time were also inversely correlated across this component.

###  Linear Classifier

```{R}
# may not need this
bioanalysis %>%
  mutate(Temp = ifelse(Temperature == "Increase", 1, 0)) -> bio_glm

# fit linear model to temperature calculated by vars
fit <- glm(Temp ~ `REM Sleep Time` + `Awake Time` + `Sleep Latency` + `Average HRV` + `Respiratory Rate` + `Steps` + `Average MET`, data=bio_glm, family="binomial")

# prediction scores for each Temperature calculated from vars
score <- predict(fit, type="response")

# compute scores for glm
class_diag(score,truth=bio_glm$Temp, positive= 1)

# confusion matrix
# increases are more likely to be predicted by this model
table(truth = bioanalysis$Temperature, prediction = score>.5)
```

```{R}
# cross-validation of linear classifier here w/ folds
k=10 #choose number of folds
data<-bio_glm[sample(nrow(bio_glm)),] #randomly order rows
folds<-cut(seq(1:nrow(bio_glm)),breaks=k,labels=F) #create 10 folds
diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-data[folds!=i,] 
  test<-data[folds==i,]
  truth<-test$Temp
  ## Train model on training set
  fit<-glm(Temp~(.)^2,data=train,family="binomial")
  probs<-predict(fit,newdata = test,type="response")
  ## Test model on test set (save all k results)
  diags<-rbind(diags,class_diag(probs,truth, positive=1))
}

summarize_all(diags,mean)
```




### Non-Parametric Classifier

```{R}
library(caret)
library(rpart)
library(rpart.plot)
 
fit <- rpart(Temperature~., data=bioanalysis)
fit

rpart.plot(fit)
```

```{R}
# cross-validation for tree
# fit <- rpart(Temperature~., data=bioanalysis, trainControl())
# class_diag(fit$pred$Increase, fit$pred$obs, positive="Increase")
```

 


### Regression/Numeric Prediction

```{R}
# using linear model to predict the actual Temperature Deviations
fit <- lm(`Temperature Deviation (°C)` ~., data=biotrends)

# predicted temperature deviations
yhat<-predict(fit)

#mean squared error
mean((biotrends$`Temperature Deviation (°C)`-yhat)^2)
```

```{R}
#cross-validation
k=5 #choose number of folds
data<-biotrends[sample(nrow(biotrends)),] #random order
folds<-cut(seq(1:nrow(biotrends)),breaks=k,labels=F) #folds
diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  ## Fit linear regression model to training set
  fit<-lm(`Temperature Deviation (°C)` ~.,data=train)
  ## Get predictions/y-hats on test set (fold i)
  yhat<-predict(fit,newdata=test)
  ## Compute prediction error  (MSE) for fold i
  diags<-mean((test$`Temperature Deviation (°C)`-yhat)^2) 
}
mean(diags)
```



### Python 

```{R}
# library(reticulate)
# 
# # create a new environment 
# conda_create("r-reticulate")
# 
# # install SciPy
# conda_install("r-reticulate", "pandas")
# # indicate that we want to use a specific condaenv
# use_condaenv("r-reticulate")
# 
# # import SciPy (will use "r-reticulate" as per call to use_condaenv)
# pd <- import("pandas")
```

```{python}


```


### Concluding Remarks

I would like to conduct a follow-up analysis with a full year's worth of data to see if the insights change with more data points. Adding another categorical variable that measures self-perceived wellness would be another interesting facet to add and potentially predict. I would hypothesize a self-rated feeling from 1-5 would show an increased likelihood of temperature deviation above the norm, negative correlation with lowered sleep latency, and positive correlation with Average Heart Rate Variability.




